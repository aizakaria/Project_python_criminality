{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5c1a64",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Load Data <a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f396f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Additional tools\n",
    "import joblib\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformed dataset\n",
    "df = pd.read_csv('Crime_Data_Transformed.csv')\n",
    "\n",
    "# Convert date columns\n",
    "df['Date Rptd'] = pd.to_datetime(df['Date Rptd'])\n",
    "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])\n",
    "\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764edfa",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering <a id='2'></a>\n",
    "\n",
    "Prepare features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for modeling\n",
    "df_model = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_cols = ['AREA NAME', 'day_name', 'month_name', 'time_period', \n",
    "                    'victim_age_group', 'location_type', 'weapon_category']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_model.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_model[f'{col}_encoded'] = le.fit_transform(df_model[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(\"‚úì Categorical variables encoded\")\n",
    "\n",
    "# Create additional features\n",
    "df_model['is_serious_crime'] = (df_model['crime_severity'] == 'Part 1 - Serious Crime').astype(int)\n",
    "df_model['is_violent'] = (df_model['crime_category'] == 'Violent Crime').astype(int)\n",
    "df_model['is_property'] = (df_model['crime_category'] == 'Property Crime').astype(int)\n",
    "\n",
    "# Time-based features\n",
    "df_model['is_night'] = ((df_model['hour'] >= 0) & (df_model['hour'] < 6)).astype(int)\n",
    "df_model['is_rush_hour'] = ((df_model['hour'] >= 7) & (df_model['hour'] <= 9) | \n",
    "                             (df_model['hour'] >= 17) & (df_model['hour'] <= 19)).astype(int)\n",
    "\n",
    "print(\"‚úì Additional features created\")\n",
    "print(f\"\\nTotal features: {df_model.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aaf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for different models\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    'Vict Age', 'AREA', 'hour', 'month', 'quarter', 'year',\n",
    "    'is_weekend', 'reporting_delay_days', 'area_risk_score',\n",
    "    'population', 'median_income', 'area_size_sq_miles',\n",
    "    'crimes_per_1000', 'is_night', 'is_rush_hour'\n",
    "]\n",
    "\n",
    "# Encoded categorical features\n",
    "encoded_features = [f'{col}_encoded' for col in categorical_cols if col in df_model.columns]\n",
    "\n",
    "# Combined features\n",
    "all_features = numerical_features + encoded_features\n",
    "\n",
    "# Filter features that exist in the dataframe\n",
    "available_features = [f for f in all_features if f in df_model.columns]\n",
    "\n",
    "print(f\"Available features for modeling: {len(available_features)}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, feat in enumerate(available_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221db76",
   "metadata": {},
   "source": [
    "## 3. Model 1: Crime Category Classification <a id='3'></a>\n",
    "\n",
    "Predict the category of crime (Violent, Property, or Other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bcc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 1: CRIME CATEGORY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "X = df_model[available_features].fillna(0)\n",
    "y = df_model['crime_category']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "results_category = {}\n",
    "\n",
    "print(\"\\nTraining classifiers...\\n\")\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results_category[name] = {\n",
    "        'model': clf,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results_category.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results_category.values()],\n",
    "    'Precision': [r['precision'] for r in results_category.values()],\n",
    "    'Recall': [r['recall'] for r in results_category.values()],\n",
    "    'F1-Score': [r['f1_score'] for r in results_category.values()]\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - CRIME CATEGORY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results_category[best_model_name]['model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   F1-Score: {results_category[best_model_name]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae88850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results for best model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, results_category[best_model_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Model Comparison\n",
    "comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(\n",
    "    kind='bar', ax=axes[1], width=0.8\n",
    ")\n",
    "axes[1].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model1_crime_category_classification.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: model1_crime_category_classification.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, results_category[best_model_name]['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e88456",
   "metadata": {},
   "source": [
    "## 4. Model 2: Crime Severity Prediction <a id='4'></a>\n",
    "\n",
    "Predict whether a crime is Part 1 (Serious) or Part 2 (Less Serious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 2: CRIME SEVERITY PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "X_severity = df_model[available_features].fillna(0)\n",
    "y_severity = df_model['is_serious_crime']\n",
    "\n",
    "# Split data\n",
    "X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(\n",
    "    X_severity, y_severity, test_size=0.2, random_state=42, stratify=y_severity\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_sev = StandardScaler()\n",
    "X_train_sev_scaled = scaler_sev.fit_transform(X_train_sev)\n",
    "X_test_sev_scaled = scaler_sev.transform(X_test_sev)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_sev.shape}\")\n",
    "print(f\"Test set: {X_test_sev.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y_train_sev.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers for severity\n",
    "classifiers_sev = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results_severity = {}\n",
    "\n",
    "print(\"\\nTraining classifiers...\\n\")\n",
    "\n",
    "for name, clf in classifiers_sev.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    clf.fit(X_train_sev_scaled, y_train_sev)\n",
    "    y_pred = clf.predict(X_test_sev_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_sev_scaled)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_sev, y_pred)\n",
    "    precision = precision_score(y_test_sev, y_pred)\n",
    "    recall = recall_score(y_test_sev, y_pred)\n",
    "    f1 = f1_score(y_test_sev, y_pred)\n",
    "    auc = roc_auc_score(y_test_sev, y_pred_proba)\n",
    "    \n",
    "    results_severity[name] = {\n",
    "        'model': clf,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_sev = pd.DataFrame({\n",
    "    'Model': list(results_severity.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results_severity.values()],\n",
    "    'Precision': [r['precision'] for r in results_severity.values()],\n",
    "    'Recall': [r['recall'] for r in results_severity.values()],\n",
    "    'F1-Score': [r['f1_score'] for r in results_severity.values()],\n",
    "    'AUC-ROC': [r['auc'] for r in results_severity.values()]\n",
    "}).sort_values('AUC-ROC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - CRIME SEVERITY PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_sev.to_string(index=False))\n",
    "\n",
    "best_model_sev = comparison_sev.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_sev}\")\n",
    "print(f\"   AUC-ROC: {results_severity[best_model_sev]['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181acf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curves\n",
    "for name in results_severity.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test_sev, results_severity[name]['probabilities'])\n",
    "    auc = results_severity[name]['auc']\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})', linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves - Severity Prediction', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix for best model\n",
    "cm = confusion_matrix(y_test_sev, results_severity[best_model_sev]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Part 2', 'Part 1'], yticklabels=['Part 2', 'Part 1'])\n",
    "axes[1].set_title(f'Confusion Matrix - {best_model_sev}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_crime_severity_prediction.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: model2_crime_severity_prediction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878f2d1",
   "metadata": {},
   "source": [
    "## 5. Model 3: Weapon Involvement Prediction <a id='5'></a>\n",
    "\n",
    "Predict whether a weapon will be involved in a crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 3: WEAPON INVOLVEMENT PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data (exclude weapon-related features from predictors)\n",
    "weapon_features = [f for f in available_features if 'weapon' not in f.lower()]\n",
    "X_weapon = df_model[weapon_features].fillna(0)\n",
    "y_weapon = df_model['weapon_involved']\n",
    "\n",
    "# Split data\n",
    "X_train_weap, X_test_weap, y_train_weap, y_test_weap = train_test_split(\n",
    "    X_weapon, y_weapon, test_size=0.2, random_state=42, stratify=y_weapon\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_weap = StandardScaler()\n",
    "X_train_weap_scaled = scaler_weap.fit_transform(X_train_weap)\n",
    "X_test_weap_scaled = scaler_weap.transform(X_test_weap)\n",
    "\n",
    "print(f\"\\nFeatures used: {len(weapon_features)}\")\n",
    "print(f\"Training set: {X_train_weap.shape}\")\n",
    "print(f\"Test set: {X_test_weap.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y_train_weap.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "classifiers_weapon = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results_weapon = {}\n",
    "\n",
    "print(\"\\nTraining classifiers...\\n\")\n",
    "\n",
    "for name, clf in classifiers_weapon.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    clf.fit(X_train_weap_scaled, y_train_weap)\n",
    "    y_pred = clf.predict(X_test_weap_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_weap_scaled)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_weap, y_pred)\n",
    "    precision = precision_score(y_test_weap, y_pred)\n",
    "    recall = recall_score(y_test_weap, y_pred)\n",
    "    f1 = f1_score(y_test_weap, y_pred)\n",
    "    auc = roc_auc_score(y_test_weap, y_pred_proba)\n",
    "    \n",
    "    results_weapon[name] = {\n",
    "        'model': clf,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_weapon = pd.DataFrame({\n",
    "    'Model': list(results_weapon.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results_weapon.values()],\n",
    "    'Precision': [r['precision'] for r in results_weapon.values()],\n",
    "    'Recall': [r['recall'] for r in results_weapon.values()],\n",
    "    'F1-Score': [r['f1_score'] for r in results_weapon.values()],\n",
    "    'AUC-ROC': [r['auc'] for r in results_weapon.values()]\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - WEAPON INVOLVEMENT PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_weapon.to_string(index=False))\n",
    "\n",
    "best_model_weapon = comparison_weapon.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_weapon}\")\n",
    "print(f\"   F1-Score: {results_weapon[best_model_weapon]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5d77c",
   "metadata": {},
   "source": [
    "## 6. Model 4: Crime Occurrence Prediction (Time Series) <a id='6'></a>\n",
    "\n",
    "Predict the number of crimes in future time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376dd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 4: CRIME OCCURRENCE PREDICTION (TIME SERIES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create daily crime counts\n",
    "df_ts = df.set_index('DATE OCC').sort_index()\n",
    "daily_crimes = df_ts.resample('D').size().reset_index()\n",
    "daily_crimes.columns = ['date', 'crime_count']\n",
    "\n",
    "# Feature engineering for time series\n",
    "daily_crimes['year'] = daily_crimes['date'].dt.year\n",
    "daily_crimes['month'] = daily_crimes['date'].dt.month\n",
    "daily_crimes['day'] = daily_crimes['date'].dt.day\n",
    "daily_crimes['dayofweek'] = daily_crimes['date'].dt.dayofweek\n",
    "daily_crimes['quarter'] = daily_crimes['date'].dt.quarter\n",
    "daily_crimes['is_weekend'] = (daily_crimes['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "# Create lag features\n",
    "for lag in [1, 7, 30]:\n",
    "    daily_crimes[f'lag_{lag}'] = daily_crimes['crime_count'].shift(lag)\n",
    "\n",
    "# Rolling averages\n",
    "daily_crimes['rolling_7'] = daily_crimes['crime_count'].rolling(window=7).mean()\n",
    "daily_crimes['rolling_30'] = daily_crimes['crime_count'].rolling(window=30).mean()\n",
    "\n",
    "# Drop rows with NaN\n",
    "daily_crimes = daily_crimes.dropna()\n",
    "\n",
    "print(f\"\\nTime series data shape: {daily_crimes.shape}\")\n",
    "print(f\"Date range: {daily_crimes['date'].min()} to {daily_crimes['date'].max()}\")\n",
    "daily_crimes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for regression\n",
    "ts_features = ['year', 'month', 'day', 'dayofweek', 'quarter', 'is_weekend',\n",
    "               'lag_1', 'lag_7', 'lag_30', 'rolling_7', 'rolling_30']\n",
    "\n",
    "X_ts = daily_crimes[ts_features]\n",
    "y_ts = daily_crimes['crime_count']\n",
    "\n",
    "# Split data (time series split)\n",
    "split_idx = int(len(X_ts) * 0.8)\n",
    "X_train_ts = X_ts[:split_idx]\n",
    "X_test_ts = X_ts[split_idx:]\n",
    "y_train_ts = y_ts[:split_idx]\n",
    "y_test_ts = y_ts[split_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train_ts.shape}\")\n",
    "print(f\"Test set: {X_test_ts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "regressors = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Linear Regression': LinearRegression()\n",
    "}\n",
    "\n",
    "results_ts = {}\n",
    "\n",
    "print(\"\\nTraining regression models...\\n\")\n",
    "\n",
    "for name, reg in regressors.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    reg.fit(X_train_ts, y_train_ts)\n",
    "    y_pred = reg.predict(X_test_ts)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_ts, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_ts, y_pred))\n",
    "    r2 = r2_score(y_test_ts, y_pred)\n",
    "    \n",
    "    results_ts[name] = {\n",
    "        'model': reg,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R¬≤: {r2:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì All regressors trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_ts = pd.DataFrame({\n",
    "    'Model': list(results_ts.keys()),\n",
    "    'MAE': [r['mae'] for r in results_ts.values()],\n",
    "    'RMSE': [r['rmse'] for r in results_ts.values()],\n",
    "    'R¬≤': [r['r2'] for r in results_ts.values()]\n",
    "}).sort_values('MAE', ascending=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - CRIME OCCURRENCE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_ts.to_string(index=False))\n",
    "\n",
    "best_model_ts = comparison_ts.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_ts}\")\n",
    "print(f\"   MAE: {results_ts[best_model_ts]['mae']:.4f}\")\n",
    "print(f\"   R¬≤: {results_ts[best_model_ts]['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot actual vs predicted for best model\n",
    "test_dates = daily_crimes.iloc[split_idx:]['date']\n",
    "axes[0].plot(test_dates, y_test_ts.values, label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(test_dates, results_ts[best_model_ts]['predictions'], \n",
    "             label='Predicted', linewidth=2, alpha=0.7)\n",
    "axes[0].set_title(f'Crime Occurrence Prediction - {best_model_ts}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Date', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Crimes', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test_ts.values - results_ts[best_model_ts]['predictions']\n",
    "axes[1].scatter(results_ts[best_model_ts]['predictions'], residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Residuals Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model4_crime_occurrence_prediction.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: model4_crime_occurrence_prediction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28a278",
   "metadata": {},
   "source": [
    "## 7. Model 5: Area Risk Score Regression <a id='7'></a>\n",
    "\n",
    "Predict the risk score of an area based on various factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c031941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 5: AREA RISK SCORE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data (exclude area_risk_score from predictors)\n",
    "risk_features = [f for f in available_features if 'risk' not in f.lower()]\n",
    "X_risk = df_model[risk_features].fillna(0)\n",
    "y_risk = df_model['area_risk_score']\n",
    "\n",
    "# Split data\n",
    "X_train_risk, X_test_risk, y_train_risk, y_test_risk = train_test_split(\n",
    "    X_risk, y_risk, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_risk.shape}\")\n",
    "print(f\"Test set: {X_test_risk.shape}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y_train_risk.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa217273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "regressors_risk = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42)\n",
    "}\n",
    "\n",
    "results_risk = {}\n",
    "\n",
    "print(\"\\nTraining regression models...\\n\")\n",
    "\n",
    "for name, reg in regressors_risk.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    reg.fit(X_train_risk, y_train_risk)\n",
    "    y_pred = reg.predict(X_test_risk)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_risk, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_risk, y_pred))\n",
    "    r2 = r2_score(y_test_risk, y_pred)\n",
    "    \n",
    "    results_risk[name] = {\n",
    "        'model': reg,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R¬≤: {r2:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì All regressors trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_risk = pd.DataFrame({\n",
    "    'Model': list(results_risk.keys()),\n",
    "    'MAE': [r['mae'] for r in results_risk.values()],\n",
    "    'RMSE': [r['rmse'] for r in results_risk.values()],\n",
    "    'R¬≤': [r['r2'] for r in results_risk.values()]\n",
    "}).sort_values('R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - AREA RISK SCORE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_risk.to_string(index=False))\n",
    "\n",
    "best_model_risk = comparison_risk.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_risk}\")\n",
    "print(f\"   R¬≤: {results_risk[best_model_risk]['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58747f3",
   "metadata": {},
   "source": [
    "## 8. Model Comparison & Evaluation <a id='8'></a>\n",
    "\n",
    "Summary of all models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        'Task': 'Crime Category Classification',\n",
    "        'Best Model': best_model_name,\n",
    "        'Metric': 'F1-Score',\n",
    "        'Score': f\"{results_category[best_model_name]['f1_score']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Task': 'Crime Severity Prediction',\n",
    "        'Best Model': best_model_sev,\n",
    "        'Metric': 'AUC-ROC',\n",
    "        'Score': f\"{results_severity[best_model_sev]['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Task': 'Weapon Involvement Prediction',\n",
    "        'Best Model': best_model_weapon,\n",
    "        'Metric': 'F1-Score',\n",
    "        'Score': f\"{results_weapon[best_model_weapon]['f1_score']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Task': 'Crime Occurrence Prediction',\n",
    "        'Best Model': best_model_ts,\n",
    "        'Metric': 'R¬≤',\n",
    "        'Score': f\"{results_ts[best_model_ts]['r2']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Task': 'Area Risk Score Prediction',\n",
    "        'Best Model': best_model_risk,\n",
    "        'Metric': 'R¬≤',\n",
    "        'Score': f\"{results_risk[best_model_risk]['r2']:.4f}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3caf0",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis <a id='9'></a>\n",
    "\n",
    "Analyze which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47847ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Crime Category model (if using tree-based)\n",
    "if hasattr(results_category[best_model_name]['model'], 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': available_features,\n",
    "        'Importance': results_category[best_model_name]['model'].feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TOP 15 MOST IMPORTANT FEATURES - {best_model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    ax.barh(range(len(top_features)), top_features['Importance'].values)\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['Feature'].values)\n",
    "    ax.set_xlabel('Importance', fontsize=12)\n",
    "    ax.set_title(f'Top 15 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Visualization saved: feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d21fbf",
   "metadata": {},
   "source": [
    "## 10. Deployment Ready Models <a id='10'></a>\n",
    "\n",
    "Save the best models for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all best models\n",
    "models_to_save = {\n",
    "    'crime_category_classifier': {\n",
    "        'model': results_category[best_model_name]['model'],\n",
    "        'scaler': scaler,\n",
    "        'features': available_features\n",
    "    },\n",
    "    'crime_severity_classifier': {\n",
    "        'model': results_severity[best_model_sev]['model'],\n",
    "        'scaler': scaler_sev,\n",
    "        'features': available_features\n",
    "    },\n",
    "    'weapon_involvement_classifier': {\n",
    "        'model': results_weapon[best_model_weapon]['model'],\n",
    "        'scaler': scaler_weap,\n",
    "        'features': weapon_features\n",
    "    },\n",
    "    'crime_occurrence_regressor': {\n",
    "        'model': results_ts[best_model_ts]['model'],\n",
    "        'features': ts_features\n",
    "    },\n",
    "    'area_risk_regressor': {\n",
    "        'model': results_risk[best_model_risk]['model'],\n",
    "        'features': risk_features\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save models\n",
    "print(\"\\nSaving models...\\n\")\n",
    "for name, model_info in models_to_save.items():\n",
    "    filename = f'{name}_model.pkl'\n",
    "    joblib.dump(model_info, filename)\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "\n",
    "# Save label encoders\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "print(\"‚úì Saved: label_encoders.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fadb8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully developed and evaluated multiple machine learning models for crime prediction:\n",
    "\n",
    "1. **Crime Category Classification**: Predicts whether a crime is violent, property, or other\n",
    "2. **Crime Severity Prediction**: Classifies crimes as serious (Part 1) or less serious (Part 2)\n",
    "3. **Weapon Involvement Prediction**: Determines if a weapon will be involved\n",
    "4. **Crime Occurrence Prediction**: Forecasts the number of crimes in future time periods\n",
    "5. **Area Risk Score Regression**: Estimates risk scores for different areas\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- Tree-based ensemble models (Random Forest, Gradient Boosting) generally performed best\n",
    "- Feature importance analysis reveals critical factors in crime prediction\n",
    "- Models achieve strong predictive performance across all tasks\n",
    "- All models are saved and ready for deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Deploy models** in production environment\n",
    "2. **Monitor performance** and retrain with new data\n",
    "3. **Create API endpoints** for real-time predictions\n",
    "4. **Integrate with Streamlit dashboard** for interactive predictions\n",
    "5. **Develop automated retraining pipeline**\n",
    "\n",
    "---\n",
    "\n",
    "**All models and visualizations have been saved for deployment and reporting.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
